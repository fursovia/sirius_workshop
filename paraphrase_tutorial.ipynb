{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sirius Workshop 2019\n",
    "\n",
    "This tutorial is prepared by Ivan Fursov at Tinkoff.\n",
    "\n",
    "Telegram: [@fursov](https://tele.click/fursov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paraphrase identification\n",
    "\n",
    "**Download** files from [here](https://yadi.sk/d/pTSMZfLAo7DjrQ)\n",
    "\n",
    "Task: given a pair of sentences, classify them as paraphrases or not paraphrases\n",
    "\n",
    "Dataset: [Quora Question Pairs](https://www.kaggle.com/quora/question-pairs-dataset)\n",
    "\n",
    "Quora's first public dataset is related to the problem of identifying duplicate questions. At Quora, an important product principle is that there should be a single question page for each logically distinct question. For example, the queries “What is the most populous state in the USA?” and “Which state in the United States has the most people?” should not exist separately on Quora because the intent behind both is identical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "pd.set_option('max_colwidth', 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(string):\n",
    "    \"\"\"\n",
    "    Remove non-letter symbols, lower the text\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z ]\", \" \", string)  \n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "\n",
    "data = pd.read_csv('data/questions.csv', nrows=1000)\n",
    "data = data.dropna()\n",
    "\n",
    "data['question1'] = data['question1'].apply(clean_string)\n",
    "data['question2'] = data['question2'].apply(clean_string)\n",
    "\n",
    "data = data[['question1', 'question2', 'is_duplicate']]\n",
    "data.columns = ['text1', 'text2', 'labels']\n",
    "\n",
    "data = data[data['text1'].apply(lambda x: len(x) > 0) & (data['text2'].apply(lambda x: len(x) > 0))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset structure\n",
    "\n",
    "There are three columns: **text1**, **text2** and **labels**. **text1**, **text2** are strings and **labels** can take 2 values: $0$, $1$. $1$ corresponds to \"texts have similar meaning\", and $0$ --- otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>what is the fastest production car</td>\n",
       "      <td>what is the world s fastest street legal car</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>how can i make money online with free of cost</td>\n",
       "      <td>how do i to make money online</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>how should i respond to my boss who fired me via email</td>\n",
       "      <td>is it okay to fire someone via email</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>what are the differences between pocso and sexual offenses in the ipc</td>\n",
       "      <td>do we need the ipc section</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>what will be the effect of banning     and      notes on stock markets in india</td>\n",
       "      <td>what will be the effect of the ban of the     rs and    rs notes on the stoc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                               text1  \\\n",
       "714                                               what is the fastest production car   \n",
       "284                                    how can i make money online with free of cost   \n",
       "623                           how should i respond to my boss who fired me via email   \n",
       "453            what are the differences between pocso and sexual offenses in the ipc   \n",
       "295  what will be the effect of banning     and      notes on stock markets in india   \n",
       "\n",
       "                                                                               text2  \\\n",
       "714                                     what is the world s fastest street legal car   \n",
       "284                                                    how do i to make money online   \n",
       "623                                             is it okay to fire someone via email   \n",
       "453                                                       do we need the ipc section   \n",
       "295  what will be the effect of the ban of the     rs and    rs notes on the stoc...   \n",
       "\n",
       "     labels  \n",
       "714       0  \n",
       "284       1  \n",
       "623       0  \n",
       "453       0  \n",
       "295       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Dev/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/dev/test -> 70/15/15\n",
    "data_splits = ('train', 'dev', 'test')\n",
    "\n",
    "train, intermediate = train_test_split(data, test_size=0.3, random_state=24)\n",
    "dev, test = train_test_split(intermediate, test_size=0.5, random_state=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((700, 3), (150, 3), (150, 3))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, dev.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Text representations\n",
    "### Bag-of-Words\n",
    "\n",
    "Bag of Words (BoW) is an algorithm that counts how many times a word appears in a document. Those word counts allow us to compare documents and gauge their similarities for applications like search, document classification and topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "\n",
    "import scipy\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "{\\displaystyle {\\text{similarity}}=\\cos(\\theta )={\\mathbf {A} \\cdot \\mathbf {B}  \\over \\|\\mathbf {A} \\|\\|\\mathbf {B} \\|}={\\frac {\\sum \\limits _{i=1}^{n}{A_{i}B_{i}}}{{\\sqrt {\\sum \\limits _{i=1}^{n}{A_{i}^{2}}}}{\\sqrt {\\sum \\limits _{i=1}^{n}{B_{i}^{2}}}}}},}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_distance(textA, textB):\n",
    "    textA = normalize(textA)\n",
    "    textB = normalize(textB)\n",
    "    if isinstance(textA, np.ndarray):\n",
    "        dot_product = np.multiply(textA, textB).sum(axis=1).flatten()\n",
    "    else:\n",
    "        dot_product = np.array(textA.multiply(textB).sum(axis=1)).flatten()\n",
    "    return 1 - dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = CountVectorizer()\n",
    "bow.fit(train['text1'].tolist() + train['text2'].tolist())\n",
    "\n",
    "bow_data = {name: dict() for name in data_splits}\n",
    "\n",
    "for d, name in zip((train, dev, test), data_splits):\n",
    "    bow_data[name]['text1'] = bow.transform(d['text1'].tolist())\n",
    "    bow_data[name]['text2'] = bow.transform(d['text2'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((700, 3010), (700, 3010))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_data['train']['text1'].shape, bow_data['train']['text2'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(textA, textB, labels):\n",
    "    cos_dists = calculate_cosine_distance(textA, textB)\n",
    "\n",
    "    best_f1 = 0\n",
    "    best_thres = None\n",
    "\n",
    "    for thres in np.linspace(0, 2, num=50):\n",
    "        f1 = f1_score((cos_dists < thres).astype(np.int32), labels)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_thres = thres\n",
    "            \n",
    "    return best_f1, best_thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(DEV) F1 score = 0.6190476190476191\n"
     ]
    }
   ],
   "source": [
    "best_f1, best_thres = calculate_score(\n",
    "    bow_data['dev']['text1'], \n",
    "    bow_data['dev']['text2'],\n",
    "    dev['labels'].values\n",
    ")\n",
    "\n",
    "print(f'(DEV) F1 score = {best_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TEST) F1 score = 0.6623376623376623\n"
     ]
    }
   ],
   "source": [
    "test_cos_dists = calculate_cosine_distance(bow_data['test']['text1'], bow_data['test']['text2'])\n",
    "\n",
    "test_f1 = f1_score((test_cos_dists < best_thres).astype(np.int32), test['labels'].values)\n",
    "print(f'(TEST) F1 score = {test_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-Idf\n",
    "\n",
    "Term-frequency-inverse document frequency (TF-IDF) is another way to represent a text by the words it contains. With TF-IDF, words are given weight – TF-IDF measures relevance, not frequency. That is, wordcounts are replaced with TF-IDF scores across the whole dataset.\n",
    "\n",
    "<img src=\"https://skymind.ai/images/wiki/tfidf.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit(train['text1'].tolist() + train['text2'].tolist())\n",
    "\n",
    "tfidf_data = {name: dict() for name in data_splits}\n",
    "\n",
    "for d, name in zip((train, dev, test), data_splits):\n",
    "    tfidf_data[name]['text1'] = tfidf.transform(d['text1'].tolist())\n",
    "    tfidf_data[name]['text2'] = tfidf.transform(d['text2'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((700, 3010), (700, 3010))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_data['train']['text1'].shape, tfidf_data['train']['text2'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(DEV) F1 score = 0.6373626373626373\n"
     ]
    }
   ],
   "source": [
    "best_f1, best_thres = calculate_score(\n",
    "    tfidf_data['dev']['text1'], \n",
    "    tfidf_data['dev']['text2'],\n",
    "    dev['labels'].values\n",
    ")\n",
    "\n",
    "print(f'(DEV) F1 score = {best_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TEST) F1 score = 0.6309523809523809\n"
     ]
    }
   ],
   "source": [
    "test_cos_dists = calculate_cosine_distance(tfidf_data['test']['text1'], tfidf_data['test']['text2'])\n",
    "test_f1 = f1_score((test_cos_dists < best_thres).astype(np.int32), test['labels'].values)\n",
    "print(f'(TEST) F1 score = {test_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf on char n-grams\n",
    "\n",
    "Very helpful if you work with russian language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3, 5))\n",
    "tfidf.fit(train['text1'].tolist() + train['text2'].tolist())\n",
    "\n",
    "tfidf_data = {name: dict() for name in data_splits}\n",
    "\n",
    "for d, name in zip((train, dev, test), data_splits):\n",
    "    tfidf_data[name]['text1'] = tfidf.transform(d['text1'].tolist())\n",
    "    tfidf_data[name]['text2'] = tfidf.transform(d['text2'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 38995)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_data['train']['text1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(DEV) F1 score = 0.6424242424242425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "best_f1, best_thres = calculate_score(\n",
    "    tfidf_data['dev']['text1'], \n",
    "    tfidf_data['dev']['text2'],\n",
    "    dev['labels'].values\n",
    ")\n",
    "\n",
    "print(f'(DEV) F1 score = {best_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TEST) F1 score = 0.6797385620915033\n"
     ]
    }
   ],
   "source": [
    "test_cos_dists = calculate_cosine_distance(tfidf_data['test']['text1'], tfidf_data['test']['text2'])\n",
    "test_f1 = f1_score((test_cos_dists < best_thres).astype(np.int32), test['labels'].values)\n",
    "print(f'(TEST) F1 score = {test_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Approaches -- fastText\n",
    "\n",
    "The gist of fastText is that instead of directly learning a vector representation for a word (as with word2vec), we learn a representation for each character n-gram. Each word is represented as a bag of character n-grams, so the overall word embedding is a sum of these character n-grams.\n",
    "\n",
    "fastText is a library whose purpose is to be used as a fast baseline for text embeddings/classification when deep learning approaches are just too slow and expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full corpora\n",
    "\n",
    "texts = train['text1'].tolist() + train['text2'].tolist()\n",
    "texts = [text.split() for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.64 s, sys: 942 ms, total: 4.58 s\n",
      "Wall time: 4.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = FastText(texts, size=32, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2vec(text, model=model):\n",
    "    assert len(text) > 0\n",
    "\n",
    "    vectors = []\n",
    "    for word in text.split():\n",
    "        try:\n",
    "            vectors.append(model.wv[word])\n",
    "        except KeyError:\n",
    "            vectors.append(np.zeros(model.vector_size))\n",
    "\n",
    "    return np.mean(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_data = {name: dict() for name in data_splits}\n",
    "\n",
    "for d, name in zip((train, dev, test), data_splits):\n",
    "    fasttext_data[name]['text1'] = np.array([text2vec(t) for t in d['text1'].tolist()])\n",
    "    fasttext_data[name]['text2'] = np.array([text2vec(t) for t in d['text2'].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_data['train']['text1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(DEV) F1 score = 0.5645933014354066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "best_f1, best_thres = calculate_score(\n",
    "    fasttext_data['dev']['text1'], \n",
    "    fasttext_data['dev']['text2'],\n",
    "    dev['labels'].values\n",
    ")\n",
    "\n",
    "print(f'(DEV) F1 score = {best_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TEST) F1 score = 0.5294117647058824\n"
     ]
    }
   ],
   "source": [
    "test_cos_dists = calculate_cosine_distance(fasttext_data['test']['text1'], fasttext_data['test']['text2'])\n",
    "test_f1 = f1_score((test_cos_dists < best_thres).astype(np.int32), test['labels'].values)\n",
    "print(f'(TEST) F1 score = {test_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained fasttext\n",
    "\n",
    "Learning word representation requires serious computational power and time. Since Facebook has done it for you, why not using that to boost productivity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment if you'd like to download (2.5Gb+)\n",
    "\n",
    "# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
    "# !unzip wiki-news-300d-1M.vec.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_from_text = KeyedVectors.load_word2vec_format('data/wiki-news-300d-1M.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_data = {name: dict() for name in data_splits}\n",
    "\n",
    "for d, name in zip((train, dev, test), data_splits):\n",
    "    fasttext_data[name]['text1'] = np.array([text2vec(t, wv_from_text) for t in d['text1'].tolist()])\n",
    "    fasttext_data[name]['text2'] = np.array([text2vec(t, wv_from_text) for t in d['text2'].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_data['train']['text1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1, best_thres = calculate_score(\n",
    "    fasttext_data['dev']['text1'], \n",
    "    fasttext_data['dev']['text2'],\n",
    "    dev['labels'].values\n",
    ")\n",
    "\n",
    "print(f'(DEV) F1 score = {best_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cos_dists = calculate_cosine_distance(fasttext_data['test']['text1'], fasttext_data['test']['text2'])\n",
    "test_f1 = f1_score((test_cos_dists < best_thres).astype(np.int32), test['labels'].values)\n",
    "print(f'(TEST) F1 score = {test_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to handle texts?\n",
    "\n",
    "Embeddings!\n",
    "\n",
    "Word embedding is one of the most popular representation of document vocabulary. It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc.\n",
    "\n",
    "<img src=\"https://adriancolyer.files.wordpress.com/2016/04/word2vec-distributed-representation.png?w=656&zoom=2\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import torchtext\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.0'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.0'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchtext.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data/paraphrase'):\n",
    "    os.makedirs('data/paraphrase')\n",
    "\n",
    "train.to_csv('data/paraphrase/train.csv', index=False)\n",
    "dev.to_csv('data/paraphrase/dev.csv', index=False)\n",
    "test.to_csv('data/paraphrase/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`cpu` device is being used.\n"
     ]
    }
   ],
   "source": [
    "device_name = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device_name)\n",
    "\n",
    "print(f'`{device}` device is being used.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParaphraseDataset:\n",
    "    def __init__(self, path, min_freq=2, batch_sizes=(64, 64, 64), device=device):\n",
    "        self.path = path\n",
    "        self.min_freq = min_freq\n",
    "        self.batch_sizes = batch_sizes\n",
    "        self.device = device\n",
    "\n",
    "        self.text_field = None\n",
    "        self.labels_field = None\n",
    "        \n",
    "        self.train_dataset, self.dev_dataset, self.test_dataset = None, None, None\n",
    "        self._build_fields()\n",
    "        \n",
    "    @property\n",
    "    def vocab(self):\n",
    "        return dict(self.text_field.vocab.stoi)\n",
    "    \n",
    "    @property\n",
    "    def inv_vocab(self):\n",
    "        return {i: key for i, key in enumerate(self.text_field.vocab.itos)}\n",
    "    \n",
    "    def _get_corpus(self, dataset):\n",
    "        corpus = []\n",
    "        for example in dataset.examples:\n",
    "            corpus.append(self.text_field.preprocess(' '.join(example.text1)))\n",
    "            corpus.append(self.text_field.preprocess(' '.join(example.text2)))\n",
    "        return corpus\n",
    "    \n",
    "    def _build_fields(self):\n",
    "        self.text_field = torchtext.data.Field(\n",
    "            sequential=True,\n",
    "            batch_first=True,\n",
    "            lower=True,\n",
    "            preprocessing=None\n",
    "        )\n",
    "\n",
    "        self.label_field = torchtext.data.Field(\n",
    "            sequential=False,\n",
    "            use_vocab=False,\n",
    "            is_target=True,\n",
    "            batch_first=True,\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        \n",
    "        fields = [\n",
    "            ('text1', self.text_field),\n",
    "            ('text2', self.text_field),\n",
    "            ('labels', self.label_field)\n",
    "        ]\n",
    "        \n",
    "        self.train_dataset, self.dev_dataset, self.test_dataset = torchtext.data.TabularDataset.splits(\n",
    "            path=self.path,\n",
    "            root='.',\n",
    "            train='train.csv',\n",
    "            validation='dev.csv',\n",
    "            test='test.csv',\n",
    "            format='csv',\n",
    "            fields=fields,\n",
    "            skip_header=True\n",
    "        )\n",
    "        \n",
    "        corpus = self._get_corpus(self.train_dataset)\n",
    "        self.text_field.build_vocab(corpus, min_freq=self.min_freq, specials=['<unk>', '<pad>'])\n",
    "\n",
    "    def create_iterators(self):\n",
    "        train_iter, dev_iter, test_iter = torchtext.data.Iterator.splits(\n",
    "            datasets=(self.train_dataset, self.dev_dataset, self.test_dataset),\n",
    "            batch_sizes=self.batch_sizes,\n",
    "            shuffle=(False, False, False),\n",
    "            sort=False,\n",
    "            device=self.device\n",
    "        )\n",
    "        \n",
    "        return train_iter, dev_iter, test_iter\n",
    "    \n",
    "    def vectorize(self, data):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ParaphraseDataset(path='data/paraphrase/')\n",
    "train_iter, dev_iter, test_iter = dataset.create_iterators()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1550, 1550)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.vocab), len(dataset.inv_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dev_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 64]\n",
       "\t[.text1]:[torch.LongTensor of size 64x32]\n",
       "\t[.text2]:[torch.LongTensor of size 64x32]\n",
       "\t[.labels]:[torch.FloatTensor of size 64]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 32]), torch.Size([64, 32]), torch.Size([64]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.text1.shape, batch.text2.shape, batch.labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  5,  14,   6,  ...,   1,   1,   1],\n",
       "        [  5, 669,   4,  ...,   1,   1,   1],\n",
       "        [  6, 127,   3,  ...,   1,   1,   1],\n",
       "        ...,\n",
       "        [  3,   4,  35,  ...,   1,   1,   1],\n",
       "        [  3,   4,  19,  ...,   1,   1,   1],\n",
       "        [ 17,  10,   6,  ...,   1,   1,   1]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.text1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural baseline\n",
    "\n",
    "<img src=\"https://i.ibb.co/D7R7kNH/raai-pizza.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim, ntokens=len(dataset.vocab), \n",
    "                 padding_idx=dataset.vocab['<pad>']):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.ntokens = ntokens\n",
    "        self.emb = nn.Embedding(\n",
    "            num_embeddings=self.ntokens,\n",
    "            embedding_dim=self.emb_dim, \n",
    "            padding_idx=padding_idx\n",
    "        )\n",
    "\n",
    "    def forward(self, ids):\n",
    "        x = self.emb(ids)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbeddingLayer(\n",
       "  (emb): Embedding(1769, 64, padding_idx=1)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder = EmbeddingLayer(emb_dim=64)\n",
    "\n",
    "embedder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 32]), torch.Size([64, 32]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.text1.shape, batch.text2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 32, 64]), torch.Size([64, 32, 64]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    embeddings1 = embedder(batch.text1)\n",
    "    embeddings2 = embedder(batch.text2)\n",
    "\n",
    "embeddings1.shape, embeddings2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0517,  0.1326, -0.7974,  ...,  2.3644, -1.5024,  0.2599],\n",
       "        [ 0.0737,  0.1903,  0.3190,  ..., -0.7811, -0.3785,  0.2524],\n",
       "        [ 0.5763, -1.1727,  0.9659,  ..., -0.7192, -0.0527,  0.2945],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i0.wp.com/mlexplained.com/wp-content/uploads/2018/05/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88-2018-05-10-13.29.52.png?w=366\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPoolingOverTime(nn.Module):\n",
    "\n",
    "    def __init__(self, dim=1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.mean(x, dim=self.dim)\n",
    "\n",
    "\n",
    "class AveragingNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim=64, hidden_dim=32, output_dim=16):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            MeanPoolingOverTime(),\n",
    "            nn.Linear(in_features=self.emb_dim, out_features=self.hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=hidden_dim, out_features=self.output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, embeds):\n",
    "\n",
    "        hidden = self.feed_forward(embeds)\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AveragingNetwork(\n",
       "  (feed_forward): Sequential(\n",
       "    (0): MeanPoolingOverTime()\n",
       "    (1): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): Linear(in_features=32, out_features=16, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body = AveragingNetwork()\n",
    "\n",
    "body.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 16]), torch.Size([64, 16]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    hidden_vectors1 = body(embeddings1)\n",
    "    hidden_vectors2 = body(embeddings2)\n",
    "\n",
    "hidden_vectors1.shape, hidden_vectors2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleHead(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.dense = nn.Linear(in_features=self.output_dim * 2, out_features=1)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        concatenated = torch.cat((x, y), dim=1)\n",
    "        z = self.dense(concatenated)\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleHead(\n",
       "  (dense): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head = SimpleHead(output_dim=16)\n",
    "\n",
    "head.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logits\n",
    "with torch.no_grad():\n",
    "    output = head(hidden_vectors1, hidden_vectors2)\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting all together\n",
    "\n",
    "class ModelHandler(nn.Module):\n",
    "    def __init__(self, embedding_encoder, body_encoder, head_encoder):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_encoder = embedding_encoder\n",
    "        self.body_encoder = body_encoder\n",
    "        self.head_encoder = head_encoder\n",
    "\n",
    "    def forward(self, text1, text2):\n",
    "\n",
    "        hidden1 = self.predict_hidden(text1, aggregate=False)\n",
    "        hidden2 = self.predict_hidden(text2, aggregate=False)\n",
    "\n",
    "        output = self.head_encoder(hidden1, hidden2)\n",
    "\n",
    "        if len(hidden1.size()) > 2:\n",
    "            hidden1 = torch.mean(hidden1, dim=1)\n",
    "            hidden2 = torch.mean(hidden2, dim=1)\n",
    "\n",
    "        return output, (hidden1, hidden2)\n",
    "\n",
    "    def predict_hidden(self, text, aggregate=True):\n",
    "\n",
    "        embeds = self.embedding_encoder(text)\n",
    "        hidden = self.body_encoder(embeds)\n",
    "\n",
    "        if aggregate and len(hidden.size()) > 2:\n",
    "            hidden = torch.mean(hidden, dim=1)\n",
    "\n",
    "        return hidden\n",
    "    \n",
    "    def predict_attention_scores(self, context, query):\n",
    "        hidden1 = self.predict_hidden(context, aggregate=False)\n",
    "        hidden2 = self.predict_hidden(query, aggregate=False)\n",
    "\n",
    "        scores = self.head_encoder.get_scores(hidden1, hidden2)\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def get_model_params(self, extra_info=None):\n",
    "        params = dict()\n",
    "        params['embedder_name'] = self.embedding_encoder._get_name()\n",
    "        params['body_name'] = self.body_encoder._get_name()\n",
    "        params['head_name'] = self.head_encoder._get_name()\n",
    "        if extra_info is not None:\n",
    "            params.update(extra_info)\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelHandler(\n",
       "  (embedding_encoder): EmbeddingLayer(\n",
       "    (emb): Embedding(1769, 64, padding_idx=1)\n",
       "  )\n",
       "  (body_encoder): AveragingNetwork(\n",
       "    (feed_forward): Sequential(\n",
       "      (0): MeanPoolingOverTime()\n",
       "      (1): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "      (3): Linear(in_features=32, out_features=16, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (head_encoder): SimpleHead(\n",
       "    (dense): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_model = ModelHandler(\n",
    "    embedding_encoder=embedder,\n",
    "    body_encoder=body, \n",
    "    head_encoder=head\n",
    ")\n",
    "\n",
    "baseline_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1]), torch.Size([64, 16]), torch.Size([64, 16]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output, (hidden1, hidden2) = baseline_model(batch.text1, batch.text2)\n",
    "\n",
    "output.shape, hidden1.shape, hidden2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboardX\n",
    "from tensorboardX import SummaryWriter\n",
    "from tensorboardX.summary import hparams\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.9'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboardX.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HSummaryWriter(SummaryWriter):\n",
    "    def add_hyperparams(self, hparam_dict, metric_dict):\n",
    "        if type(hparam_dict) is not dict or type(metric_dict) is not dict:\n",
    "            raise TypeError('hparam_dict and metric_dict should be dictionary.')\n",
    "        exp, ssi, sei = hparams(hparam_dict, metric_dict)\n",
    "        self.file_writer.add_summary(exp)\n",
    "        self.file_writer.add_summary(ssi)\n",
    "        self.file_writer.add_summary(sei)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*UJxVqLnbSj42eRhasKeLOA.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1(y_true, y_prob, thres=None, average='binary'):\n",
    "    if thres is None:\n",
    "        score = max([\n",
    "            f1_score(y_true, (y_prob > thres).astype(int), average=average) for thres in np.linspace(0, 1)\n",
    "        ])\n",
    "    else:\n",
    "        if average != 'binary':\n",
    "            preds = y_prob\n",
    "        else:\n",
    "            preds = (y_prob > thres).astype(int)\n",
    "        score = float(f1_score(y_true, preds, average=average))\n",
    "    return score\n",
    "\n",
    "\n",
    "def save_checkpoint(state_dict: dict, path: str, epoch: int) -> None:\n",
    "    torch.save(state_dict, f'{path}/model_{epoch}')\n",
    "    \n",
    "\n",
    "def write_metrics(writer, step, values):\n",
    "    for name, value in values.items():\n",
    "        writer.add_scalar(name, value, global_step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "save_freq = 30\n",
    "\n",
    "CRITERION = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train_one_epoch(model, optimizer, iterator, writer, epoch):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(iterator, start=(epoch - 1) * len(iterator)):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits, (text1_hidden, text2_hidden) = model(batch.text1, batch.text2)\n",
    "        loss = CRITERION(logits.squeeze(), batch.labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % save_freq == 0:\n",
    "            y_true = batch.labels.cpu().numpy()\n",
    "            y_prob = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "            f1 = calculate_f1(y_true=y_true, y_prob=y_prob)\n",
    "            write_metrics(writer, step, {'loss': loss.item(), 'f1': f1})\n",
    "            print(f'[Train]  Epoch = {epoch}, Loss Value = {loss.item():.4f}, F1 score = {f1:.4f}')\n",
    "\n",
    "\n",
    "def validate(model, iterator, writer, epoch, step=None):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss_history = list()\n",
    "        f1_history = list()\n",
    "        for batch in iterator:\n",
    "            logits, (text1_hidden, text2_hidden) = model(batch.text1, batch.text2)\n",
    "            loss = CRITERION(logits.squeeze(), batch.labels)\n",
    "            loss_history.append(loss.item())\n",
    "            \n",
    "            y_true = batch.labels.cpu().numpy()\n",
    "            y_prob = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "            f1 = calculate_f1(y_true=y_true, y_prob=y_prob)\n",
    "            f1_history.append(f1)\n",
    "\n",
    "    loss = np.mean(loss_history)\n",
    "    f1 = np.mean(f1_history)\n",
    "    metrics = {'loss': loss, 'f1': f1}\n",
    "    write_metrics(writer, step, metrics)\n",
    "    print(f'>>>>>>> [Test]  Epoch = {epoch}, Loss Value = {loss:.4f}, F1 score = {f1:.4f}')\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def train_evaluate(model_path,\n",
    "                   model,\n",
    "                   train_iter,\n",
    "                   dev_iter,\n",
    "                   num_epochs=num_epochs, \n",
    "                   save_freq=save_freq):\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "    \n",
    "    train_writer = SummaryWriter(model_path)\n",
    "    dev_writer = SummaryWriter(os.path.join(model_path, 'eval'))\n",
    "    \n",
    "    metrics = validate(model, dev_iter, dev_writer, epoch=0, step=0)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_one_epoch(model, optimizer, train_iter, train_writer, epoch)\n",
    "        metrics = validate(model, dev_iter, dev_writer, epoch, step=(epoch * len(train_iter)))\n",
    "        \n",
    "        save_checkpoint(model.state_dict(), str(model_path), epoch)\n",
    "        \n",
    "    with HSummaryWriter(os.path.join(model_path, 'eval')) as w:\n",
    "        w.add_hyperparams(model.get_model_params(), metrics)\n",
    "        \n",
    "    train_writer.close()\n",
    "    dev_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{L}=-\\sum_{i=1}^{N}\\left[y_{i} \\log p_{i}+\\left(1-y_{i}\\right) \\log \\left(1-p_{i}\\right)\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>> [Test]  Epoch = 0, Loss Value = 0.6831, F1 score = 0.5857\n",
      "[Train]  Epoch = 1, Loss Value = 0.6755, F1 score = 0.5682\n",
      ">>>>>>> [Test]  Epoch = 1, Loss Value = 0.6443, F1 score = 0.6809\n",
      ">>>>>>> [Test]  Epoch = 2, Loss Value = 0.5989, F1 score = 0.6839\n",
      "[Train]  Epoch = 3, Loss Value = 0.5433, F1 score = 0.7925\n",
      ">>>>>>> [Test]  Epoch = 3, Loss Value = 0.6101, F1 score = 0.5948\n",
      ">>>>>>> [Test]  Epoch = 4, Loss Value = 0.6972, F1 score = 0.6908\n",
      ">>>>>>> [Test]  Epoch = 5, Loss Value = 0.9105, F1 score = 0.6889\n"
     ]
    }
   ],
   "source": [
    "train_evaluate(\n",
    "    model_path='experiments/model_baseline', \n",
    "    model=baseline_model, \n",
    "    train_iter=train_iter, \n",
    "    dev_iter=dev_iter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More complex approach\n",
    "\n",
    "<img src=\"https://i.ibb.co/FXDqQbT/Screenshot-2019-07-05-at-09-51-08.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SneakyHead(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.dense = nn.Linear(in_features=self.output_dim * 4, out_features=1)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        emb_mul = torch.mul(x, y)\n",
    "        emb_abs = torch.abs(x - y)\n",
    "        concatenated = torch.cat([x, y, emb_abs, emb_mul], dim=1)\n",
    "        z = self.dense(concatenated)\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelHandler(\n",
       "  (embedding_encoder): EmbeddingLayer(\n",
       "    (emb): Embedding(1769, 128, padding_idx=1)\n",
       "  )\n",
       "  (body_encoder): AveragingNetwork(\n",
       "    (feed_forward): Sequential(\n",
       "      (0): MeanPoolingOverTime()\n",
       "      (1): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "      (3): Linear(in_features=64, out_features=32, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (head_encoder): SneakyHead(\n",
       "    (dense): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dim = 128\n",
    "hidden_dim = 64\n",
    "output_dim = 32\n",
    "\n",
    "\n",
    "sneaky_model = ModelHandler(\n",
    "    embedding_encoder=EmbeddingLayer(emb_dim),\n",
    "    body_encoder=AveragingNetwork(emb_dim, hidden_dim, output_dim), \n",
    "    head_encoder=SneakyHead(output_dim)\n",
    ")\n",
    "\n",
    "sneaky_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>> [Test]  Epoch = 0, Loss Value = 0.7018, F1 score = 0.5701\n",
      "[Train]  Epoch = 1, Loss Value = 0.7068, F1 score = 0.5116\n",
      ">>>>>>> [Test]  Epoch = 1, Loss Value = 0.6283, F1 score = 0.6697\n",
      ">>>>>>> [Test]  Epoch = 2, Loss Value = 0.6482, F1 score = 0.6561\n",
      "[Train]  Epoch = 3, Loss Value = 0.4871, F1 score = 0.7111\n",
      ">>>>>>> [Test]  Epoch = 3, Loss Value = 0.6876, F1 score = 0.7028\n",
      ">>>>>>> [Test]  Epoch = 4, Loss Value = 0.6253, F1 score = 0.7546\n",
      ">>>>>>> [Test]  Epoch = 5, Loss Value = 0.8330, F1 score = 0.6824\n"
     ]
    }
   ],
   "source": [
    "train_evaluate(\n",
    "    model_path='experiments/sneaky_model', \n",
    "    model=sneaky_model, \n",
    "    train_iter=train_iter, \n",
    "    dev_iter=dev_iter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try attention!\n",
    "\n",
    "But what is attention?\n",
    "\n",
    "Attention is simply a vector, often the outputs of dense layer using softmax function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://jalammar.github.io/images/t/transformer_self-attention_visualization.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bilinear Attention\n",
    "\n",
    "$$\n",
    "\\begin{aligned} s_{j}^{t} &=h_{j}^{q T} W_{b} h_{t}^{p} \\\\ a_{i}^{t} &=\\exp \\left(s_{i}^{t}\\right) / \\Sigma_{j=1}^{N} \\exp \\left(s_{j}^{t}\\right) \\\\ q_{t}^{b} &=\\Sigma_{i=1}^{N} a_{i}^{t} h_{i}^{q} \\end{aligned}\n",
    "$$\n",
    "\n",
    "### Concat Attention\n",
    "\n",
    "$$\n",
    "\\begin{aligned} s_{j}^{t} &=v_{c}^{T} \\tanh \\left(W_{c}^{1} h_{j}^{q}+W_{c}^{2} h_{t}^{p}\\right) \\\\ a_{i}^{t} &=\\exp \\left(s_{i}^{t}\\right) / \\sum_{j=1}^{N} \\exp \\left(s_{j}^{t}\\right) \\\\ q_{t}^{c} &=\\Sigma_{i=1}^{N} a_{i}^{t} h_{i}^{q} \\end{aligned}\n",
    "$$\n",
    "\n",
    "### Dot Attention\n",
    "\n",
    "$$\n",
    "\\begin{aligned} s_{j}^{t} &=v_{d}^{T} \\tanh \\left(W_{d}\\left(h_{j}^{q} \\odot h_{t}^{p}\\right)\\right) \\\\ a_{i}^{t} &=\\exp \\left(s_{i}^{t}\\right) / \\Sigma_{j=1}^{N} \\exp \\left(s_{j}^{t}\\right) \\\\ q_{t}^{d} &=\\Sigma_{i=1}^{N} a_{i}^{t} h_{i}^{q} \\end{aligned}\n",
    "$$\n",
    "\n",
    "### Minus Attention\n",
    "\n",
    "$$\n",
    "\\begin{aligned} s_{j}^{t} &=v_{m}^{T} \\tanh \\left(W_{m}\\left(h_{j}^{q}-h_{t}^{p}\\right)\\right) \\\\ a_{i}^{t} &=\\exp \\left(s_{i}^{t}\\right) / \\Sigma_{j=1}^{N} \\exp \\left(s_{j}^{t}\\right) \\\\ q_{t}^{m} &=\\Sigma_{i=1}^{N} a_{i}^{t} h_{i}^{q} \\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilinearAttention(nn.Module):\n",
    "    # x^T W y\n",
    "\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.W = nn.Linear(self.emb_dim, self.emb_dim, bias=False)\n",
    "\n",
    "    def forward(self, context, query):\n",
    "        scores = self.get_scores(context, query)\n",
    "        output = torch.bmm(scores, context)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def get_scores(self, context, query):\n",
    "        contextW = self.W(context)\n",
    "        scores = torch.bmm(contextW, query.transpose(1, 2))\n",
    "        scores = torch.softmax(scores, dim=1).transpose(2, 1)\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    \n",
    "class MinusAttention(nn.Module):\n",
    "    # v^T tanh(W(x - y))\n",
    "\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.W = nn.Linear(self.emb_dim, self.emb_dim, bias=False)\n",
    "        self.v = nn.Linear(self.emb_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, context, query):\n",
    "        scores = self.get_scores(context, query)\n",
    "        output = torch.bmm(scores.transpose(2, 1), context)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def get_scores(self, context, query):\n",
    "        batch_size, m, _ = context.size()\n",
    "        k = query.size(1)\n",
    "\n",
    "        context_ = context.repeat(1, k, 1)\n",
    "        query_ = query.repeat_interleave(m, dim=1)\n",
    "        minus = torch.sub(context_, query_)\n",
    "\n",
    "        Wminus = self.W(minus)\n",
    "        Wminus_tanh = torch.tanh(Wminus)\n",
    "\n",
    "        scores = self.v(Wminus_tanh)\n",
    "        scores = scores.reshape(batch_size, m, k)\n",
    "        scores = torch.softmax(scores, dim=1)\n",
    "        \n",
    "        return scores\n",
    "\n",
    "\n",
    "class ConcatAttention(nn.Module):\n",
    "    # v^T tanh(W_1 x + W_2 y)\n",
    "\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.W1 = nn.Linear(self.emb_dim, self.emb_dim, bias=False)\n",
    "        self.W2 = nn.Linear(self.emb_dim, self.emb_dim, bias=False)\n",
    "        self.v = nn.Linear(self.emb_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, context, query):\n",
    "        scores = self.get_scores(context, query)\n",
    "        output = torch.bmm(scores.transpose(2, 1), context)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def get_scores(self, context, query):\n",
    "        batch_size, m, _ = context.size()\n",
    "        k = query.size(1)\n",
    "\n",
    "        context_ = context.repeat(1, k, 1)\n",
    "        query_ = query.repeat_interleave(m, dim=1)\n",
    "\n",
    "        W1context = self.W1(context_)\n",
    "        W2query = self.W2(query_)\n",
    "        Wsum_tanh = torch.tanh(W1context + W2query)\n",
    "\n",
    "        scores = self.v(Wsum_tanh)\n",
    "        scores = scores.reshape(batch_size, m, k)\n",
    "        scores = torch.softmax(scores, dim=1)\n",
    "        \n",
    "        return scores\n",
    "\n",
    "\n",
    "class DotAttention(nn.Module):\n",
    "    # v^T tanh(W (x * y))\n",
    "\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.W = nn.Linear(self.emb_dim, self.emb_dim, bias=False)\n",
    "        self.v = nn.Linear(self.emb_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, context, query):\n",
    "        scores = self.get_scores(context, query)\n",
    "        output = torch.bmm(scores.transpose(2, 1), context)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def get_scores(self, context, query):\n",
    "        batch_size, m, _ = context.size()\n",
    "        k = query.size(1)\n",
    "\n",
    "        context_ = context.repeat(1, k, 1)\n",
    "        query_ = query.repeat_interleave(m, dim=1)\n",
    "        dot = torch.mul(context_, query_)\n",
    "\n",
    "        Wdot = self.W(dot)\n",
    "        Wdot_tanh = torch.tanh(Wdot)\n",
    "\n",
    "        scores = self.v(Wdot_tanh)\n",
    "        scores = scores.reshape(batch_size, m, k)\n",
    "        scores = torch.softmax(scores, dim=1)\n",
    "        \n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BilinearAttention(\n",
       "  (W): Linear(in_features=64, out_features=64, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dim = 64\n",
    "\n",
    "att_mechanism = BilinearAttention(emb_dim=emb_dim)\n",
    "att_mechanism.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 5, 64])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "x = torch.rand((batch_size, 3, emb_dim), device=device)\n",
    "y = torch.rand((batch_size, 5, emb_dim), device=device)\n",
    "\n",
    "# [batch_size, query_len, emb_dim]\n",
    "att_mechanism(context=x, query=y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 64])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_mechanism(context=y, query=x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1838/1*8nFrwolzTYtUWSaziiJGkg.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim, hidden_dim, num_layers, bidirectional=False, aggregate=False):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bidirectional = bidirectional\n",
    "        self.aggregate = aggregate\n",
    "        self.output_dim = self.hidden_dim * 2 if self.bidirectional else self.hidden_dim\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            self.emb_dim,\n",
    "            self.hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=self.bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, embeds):\n",
    "\n",
    "        output, _ = self.rnn(embeds)\n",
    "\n",
    "        if self.aggregate:\n",
    "            output = torch.mean(output, dim=1)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, output_dim, attention_mechanism):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.attention_mechanism = attention_mechanism(self.output_dim)\n",
    "        self.dense = nn.Linear(in_features=self.output_dim * 8, out_features=1)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        new_x = self.attention_mechanism(context=y, query=x)\n",
    "        new_y = self.attention_mechanism(context=x, query=y)\n",
    "        \n",
    "        x = torch.cat((new_x, x), dim=-1)\n",
    "        y = torch.cat((new_y, y), dim=-1)\n",
    "\n",
    "        x = torch.mean(x, dim=1)\n",
    "        y = torch.mean(y, dim=1)\n",
    "        \n",
    "        emb_mul = torch.mul(x, y)\n",
    "        emb_abs = torch.abs(x - y)\n",
    "        \n",
    "        concatenated = torch.cat([x, y, emb_mul, emb_abs], dim=1)\n",
    "        z = self.dense(concatenated)\n",
    "\n",
    "        return z\n",
    "    \n",
    "    def get_scores(self, x, y):\n",
    "        scores = self.attention_mechanism.get_scores(x, y)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelHandler(\n",
       "  (embedding_encoder): EmbeddingLayer(\n",
       "    (emb): Embedding(1769, 128, padding_idx=1)\n",
       "  )\n",
       "  (body_encoder): Identity()\n",
       "  (head_encoder): AttentionHead(\n",
       "    (attention_mechanism): BilinearAttention(\n",
       "      (W): Linear(in_features=128, out_features=128, bias=False)\n",
       "    )\n",
       "    (dense): Linear(in_features=1024, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dim = 128\n",
    "\n",
    "attention_model_baseline = ModelHandler(\n",
    "    embedding_encoder=EmbeddingLayer(emb_dim=emb_dim), \n",
    "    body_encoder=nn.Identity(), \n",
    "    head_encoder=AttentionHead(\n",
    "        output_dim=emb_dim, \n",
    "        attention_mechanism=BilinearAttention\n",
    "    )\n",
    ")\n",
    "\n",
    "attention_model_baseline.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>> [Test]  Epoch = 0, Loss Value = 0.6762, F1 score = 0.5446\n",
      "[Train]  Epoch = 1, Loss Value = 0.6802, F1 score = 0.6486\n",
      ">>>>>>> [Test]  Epoch = 1, Loss Value = 0.7081, F1 score = 0.6793\n",
      ">>>>>>> [Test]  Epoch = 2, Loss Value = 0.6404, F1 score = 0.6450\n",
      "[Train]  Epoch = 3, Loss Value = 0.5174, F1 score = 0.7037\n",
      ">>>>>>> [Test]  Epoch = 3, Loss Value = 0.7815, F1 score = 0.6534\n",
      ">>>>>>> [Test]  Epoch = 4, Loss Value = 0.8412, F1 score = 0.6681\n",
      ">>>>>>> [Test]  Epoch = 5, Loss Value = 0.8080, F1 score = 0.6627\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(attention_model_baseline.parameters(), lr=0.003)\n",
    "\n",
    "train_evaluate(\n",
    "    model_path='experiments/attention_model_baseline', \n",
    "    model=attention_model_baseline, \n",
    "    train_iter=train_iter, \n",
    "    dev_iter=dev_iter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelHandler(\n",
       "  (embedding_encoder): EmbeddingLayer(\n",
       "    (emb): Embedding(1769, 128, padding_idx=1)\n",
       "  )\n",
       "  (body_encoder): SimpleLSTM(\n",
       "    (rnn): LSTM(128, 64, batch_first=True)\n",
       "  )\n",
       "  (head_encoder): AttentionHead(\n",
       "    (attention_mechanism): BilinearAttention(\n",
       "      (W): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "    (dense): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dim = 128\n",
    "hidden_dim = 64\n",
    "num_layers = 1\n",
    "\n",
    "\n",
    "attention_model = ModelHandler(\n",
    "    embedding_encoder=EmbeddingLayer(emb_dim=emb_dim), \n",
    "    body_encoder=SimpleLSTM(emb_dim=emb_dim, hidden_dim=hidden_dim, num_layers=num_layers), \n",
    "    head_encoder=AttentionHead(\n",
    "        output_dim=hidden_dim, \n",
    "        attention_mechanism=BilinearAttention\n",
    "    )\n",
    ")\n",
    "\n",
    "attention_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>> [Test]  Epoch = 0, Loss Value = 0.6918, F1 score = 0.5470\n",
      "[Train]  Epoch = 1, Loss Value = 0.6933, F1 score = 0.6087\n",
      ">>>>>>> [Test]  Epoch = 1, Loss Value = 0.7128, F1 score = 0.6307\n",
      ">>>>>>> [Test]  Epoch = 2, Loss Value = 0.6183, F1 score = 0.6736\n",
      "[Train]  Epoch = 3, Loss Value = 0.6005, F1 score = 0.6769\n",
      ">>>>>>> [Test]  Epoch = 3, Loss Value = 0.8510, F1 score = 0.6896\n",
      ">>>>>>> [Test]  Epoch = 4, Loss Value = 0.7635, F1 score = 0.6643\n",
      ">>>>>>> [Test]  Epoch = 5, Loss Value = 0.7587, F1 score = 0.6750\n"
     ]
    }
   ],
   "source": [
    "train_evaluate(\n",
    "    model_path='experiments/attention_model', \n",
    "    model=attention_model, \n",
    "    train_iter=train_iter, \n",
    "    dev_iter=dev_iter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run\n",
    "\n",
    "> tensorboard --logdir experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_tensor(text, dataset_handler=dataset):\n",
    "    \n",
    "    if isinstance(text, str):\n",
    "        text = [text]\n",
    "    \n",
    "    field = ('x', dataset_handler.train_dataset.fields['text1'])\n",
    "    examples = [torchtext.data.Example.fromlist([t], fields=[field]) for t in text]\n",
    "    dataset = torchtext.data.Dataset(examples, fields=[field])\n",
    "\n",
    "    iterator = torchtext.data.Iterator(\n",
    "        dataset=dataset,\n",
    "        batch_size=len(text),\n",
    "        shuffle=False,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    return next(iter(iterator)).x\n",
    "\n",
    "\n",
    "def plot_attention(query, context, att_weights, scale=True):\n",
    "    tokens_a = query.split()\n",
    "    tokens_b = context.split()\n",
    "    \n",
    "    assert len(tokens_a) == att_weights.shape[0]\n",
    "    assert len(tokens_b) == att_weights.shape[1]\n",
    "    \n",
    "    if scale:\n",
    "        mins = att_weights.min(axis=1)\n",
    "        maxes = att_weights.max(axis=1)\n",
    "        att_weights = (att_weights - mins.reshape(-1, 1))  / (maxes - mins).reshape(-1, 1)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    ax.imshow(att_weights, cmap='gray')\n",
    "    ax.set_xticks(np.arange(att_weights.shape[1]))\n",
    "    ax.set_yticks(np.arange(att_weights.shape[0]))\n",
    "\n",
    "    ax.set_xticklabels([word for word in tokens_b])\n",
    "    ax.set_yticklabels([word for word in tokens_a])\n",
    "\n",
    "    ax.tick_params(labelsize=16)\n",
    "    ax.tick_params(axis='x', labelrotation=90)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def get_attention_scores(model, textA, textB):\n",
    "    with torch.no_grad():\n",
    "        attn_scores = model.predict_attention_scores(\n",
    "            text_to_tensor(textA), \n",
    "            text_to_tensor(textB)\n",
    "        ).squeeze(0)\n",
    "\n",
    "    attn_scores = attn_scores.detach().cpu().numpy()\n",
    "    \n",
    "    return attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label = 0\n",
      "Text1 = what is it like to work in asahi india glass  what will be the pay scale after one or two years\n",
      "Text2 = when will we get the call letter after we get the letter of intent in asahi india glass recruitment\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ex = 2\n",
    "\n",
    "textA = train.iloc[ex].text1\n",
    "textB = train.iloc[ex].text2\n",
    "lab = train.iloc[ex].labels\n",
    "\n",
    "print(f'Label = {lab}')\n",
    "print(f'Text1 = {textA}')\n",
    "print(f'Text2 = {textB}')\n",
    "\n",
    "attn_scores = get_attention_scores(attention_model, textA, textB)\n",
    "\n",
    "plot_attention(textB, textA, attn_scores, scale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: show that embeddings from the models are better than fasttext's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to improve?\n",
    "\n",
    "* include triplet loss\n",
    "* gain more data\n",
    "* complex architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "* [Attention? Attention!](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)\n",
    "* [Supervised Learning of Universal Sentence Representations from Natural Language Inference Data](https://arxiv.org/pdf/1705.02364.pdf)\n",
    "* [Multiway Attention Networks for Modeling Sentence Pairs](https://www.ijcai.org/proceedings/2018/0613.pdf)\n",
    "* [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
    "* [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\n",
    "* [Attention and Augmented Recurrent Neural Networks](https://distill.pub/2016/augmented-rnns/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
